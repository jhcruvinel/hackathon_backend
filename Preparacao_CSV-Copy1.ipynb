{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install psycopg2\n",
    "#!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports iniciais\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dados_teste.csv',delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4721.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2818.420545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2219.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>4693.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7108.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9995.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            numero\n",
       "count   400.000000\n",
       "mean   4721.890000\n",
       "std    2818.420545\n",
       "min      24.000000\n",
       "25%    2219.500000\n",
       "50%    4693.500000\n",
       "75%    7108.750000\n",
       "max    9995.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nome        400\n",
       "empresa     400\n",
       "endereco    400\n",
       "data1       400\n",
       "data2       400\n",
       "numero      400\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>empresa</th>\n",
       "      <th>endereco</th>\n",
       "      <th>data1</th>\n",
       "      <th>data2</th>\n",
       "      <th>numero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Anjolie Q. Campbell</td>\n",
       "      <td>Luctus Company</td>\n",
       "      <td>P.O. Box 974, 4866 Integer Av.</td>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>2018-11-03</td>\n",
       "      <td>5796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Deanna F. Burnett</td>\n",
       "      <td>Quisque Nonummy Consulting</td>\n",
       "      <td>504-1917 Urna Avenue</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>7320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Cole Z. Travis</td>\n",
       "      <td>Malesuada Fames Ac Associates</td>\n",
       "      <td>P.O. Box 627, 3320 Aliquam, Road</td>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Sonya Y. Mckinney</td>\n",
       "      <td>Mauris Non LLP</td>\n",
       "      <td>Ap #935-8507 Pede Rd.</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>2019-12-04</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Logan V. Holmes</td>\n",
       "      <td>Leo Cras Foundation</td>\n",
       "      <td>Ap #235-1628 Consectetuer Rd.</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>2019-02-03</td>\n",
       "      <td>6218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nome                        empresa  \\\n",
       "0  Anjolie Q. Campbell                 Luctus Company   \n",
       "1    Deanna F. Burnett     Quisque Nonummy Consulting   \n",
       "2       Cole Z. Travis  Malesuada Fames Ac Associates   \n",
       "3    Sonya Y. Mckinney                 Mauris Non LLP   \n",
       "4      Logan V. Holmes            Leo Cras Foundation   \n",
       "\n",
       "                           endereco       data1       data2  numero  \n",
       "0    P.O. Box 974, 4866 Integer Av.  2018-10-23  2018-11-03    5796  \n",
       "1              504-1917 Urna Avenue  2018-12-01  2020-06-10    7320  \n",
       "2  P.O. Box 627, 3320 Aliquam, Road  2020-08-15  2020-09-02    7639  \n",
       "3             Ap #935-8507 Pede Rd.  2018-10-13  2019-12-04      82  \n",
       "4     Ap #235-1628 Consectetuer Rd.  2019-07-18  2019-02-03    6218  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "ignore = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']','``',':','http','html','//members']\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Cria o Stemmer\n",
    "stemmer = SnowballStemmer('portuguese')\n",
    "def preprocess_text(content):\n",
    "    content.translate(str.maketrans('', '', string.punctuation))\n",
    "    word_tokens = nltk.word_tokenize(content)\n",
    "    #print(len(word_tokens),word_tokens)\n",
    "    word_tokens2 = [w.lower() for w in word_tokens if w not in portuguese_stops]\n",
    "    #print(len(word_tokens2),word_tokens2)\n",
    "    word_tokens3 = [w for w in word_tokens2 if w not in ignore]\n",
    "    #print(len(word_tokens3),word_tokens3)\n",
    "    word_tokens4 = [stemmer.stem(w) for w in word_tokens3]\n",
    "    #print(len(word_tokens4),word_tokens4)\n",
    "    #return word_tokens4\n",
    "    return ' '.join(word_tokens4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = 'Teste de senten√ßa para tokenizar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'sentenc', 'tokeniz']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nome'] = df['nome'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install sklearn\n",
    "#!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [anjoli, q., campbell]\n",
      "1        [deann, f., burnett]\n",
      "2             [col, z., trav]\n",
      "3        [sony, y., mckinney]\n",
      "4           [logan, v., holm]\n",
      "                ...          \n",
      "395      [olga, i., thornton]\n",
      "396           [xen, k., stok]\n",
      "397      [cassidy, k., potts]\n",
      "398          [kyle, p., robl]\n",
      "399        [jarrod, n., robl]\n",
      "Name: nome, Length: 400, dtype: object\n"
     ]
    }
   ],
   "source": [
    "texts = df['nome']\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cnt = len(dictionary.token2id)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = preprocess_text('Vanna da silva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword is similar to text1: 0.0000\n",
      "keyword is similar to text2: 0.0000\n",
      "keyword is similar to text3: 0.0000\n",
      "keyword is similar to text4: 0.0000\n",
      "keyword is similar to text5: 0.0000\n",
      "keyword is similar to text6: 0.0000\n",
      "keyword is similar to text7: 0.0000\n",
      "keyword is similar to text8: 0.0000\n",
      "keyword is similar to text9: 0.0000\n",
      "keyword is similar to text10: 0.0000\n",
      "keyword is similar to text11: 0.0000\n",
      "keyword is similar to text12: 0.0000\n",
      "keyword is similar to text13: 0.0000\n",
      "keyword is similar to text14: 0.0000\n",
      "keyword is similar to text15: 0.0000\n",
      "keyword is similar to text16: 0.0000\n",
      "keyword is similar to text17: 0.0000\n",
      "keyword is similar to text18: 0.0000\n",
      "keyword is similar to text19: 0.0000\n",
      "keyword is similar to text20: 0.0000\n",
      "keyword is similar to text21: 0.0000\n",
      "keyword is similar to text22: 0.0000\n",
      "keyword is similar to text23: 0.0000\n",
      "keyword is similar to text24: 0.0000\n",
      "keyword is similar to text25: 0.0000\n",
      "keyword is similar to text26: 0.0000\n",
      "keyword is similar to text27: 0.0000\n",
      "keyword is similar to text28: 0.0000\n",
      "keyword is similar to text29: 0.0000\n",
      "keyword is similar to text30: 0.0000\n",
      "keyword is similar to text31: 0.0000\n",
      "keyword is similar to text32: 0.0000\n",
      "keyword is similar to text33: 0.0000\n",
      "keyword is similar to text34: 0.0000\n",
      "keyword is similar to text35: 0.0000\n",
      "keyword is similar to text36: 0.0000\n",
      "keyword is similar to text37: 0.0000\n",
      "keyword is similar to text38: 0.0000\n",
      "keyword is similar to text39: 0.0000\n",
      "keyword is similar to text40: 0.0000\n",
      "keyword is similar to text41: 0.0000\n",
      "keyword is similar to text42: 0.0000\n",
      "keyword is similar to text43: 0.0000\n",
      "keyword is similar to text44: 0.0000\n",
      "keyword is similar to text45: 0.0000\n",
      "keyword is similar to text46: 0.0000\n",
      "keyword is similar to text47: 0.0000\n",
      "keyword is similar to text48: 0.0000\n",
      "keyword is similar to text49: 0.0000\n",
      "keyword is similar to text50: 0.0000\n",
      "keyword is similar to text51: 0.0000\n",
      "keyword is similar to text52: 0.0000\n",
      "keyword is similar to text53: 0.0000\n",
      "keyword is similar to text54: 0.0000\n",
      "keyword is similar to text55: 0.0000\n",
      "keyword is similar to text56: 0.0000\n",
      "keyword is similar to text57: 0.0000\n",
      "keyword is similar to text58: 0.0000\n",
      "keyword is similar to text59: 0.0000\n",
      "keyword is similar to text60: 0.0000\n",
      "keyword is similar to text61: 0.0000\n",
      "keyword is similar to text62: 0.0000\n",
      "keyword is similar to text63: 0.0000\n",
      "keyword is similar to text64: 0.0000\n",
      "keyword is similar to text65: 0.0000\n",
      "keyword is similar to text66: 0.0000\n",
      "keyword is similar to text67: 0.0000\n",
      "keyword is similar to text68: 0.0000\n",
      "keyword is similar to text69: 0.0000\n",
      "keyword is similar to text70: 0.0000\n",
      "keyword is similar to text71: 0.0000\n",
      "keyword is similar to text72: 0.0000\n",
      "keyword is similar to text73: 0.0000\n",
      "keyword is similar to text74: 0.0000\n",
      "keyword is similar to text75: 0.0000\n",
      "keyword is similar to text76: 0.0000\n",
      "keyword is similar to text77: 0.0000\n",
      "keyword is similar to text78: 0.0000\n",
      "keyword is similar to text79: 0.0000\n",
      "keyword is similar to text80: 0.0000\n",
      "keyword is similar to text81: 0.0000\n",
      "keyword is similar to text82: 0.0000\n",
      "keyword is similar to text83: 0.0000\n",
      "keyword is similar to text84: 0.0000\n",
      "keyword is similar to text85: 0.0000\n",
      "keyword is similar to text86: 0.0000\n",
      "keyword is similar to text87: 0.0000\n",
      "keyword is similar to text88: 0.0000\n",
      "keyword is similar to text89: 0.0000\n",
      "keyword is similar to text90: 0.0000\n",
      "keyword is similar to text91: 0.0000\n",
      "keyword is similar to text92: 0.0000\n",
      "keyword is similar to text93: 0.0000\n",
      "keyword is similar to text94: 0.0000\n",
      "keyword is similar to text95: 0.0000\n",
      "keyword is similar to text96: 0.0000\n",
      "keyword is similar to text97: 0.0000\n",
      "keyword is similar to text98: 0.0000\n",
      "keyword is similar to text99: 0.0000\n",
      "keyword is similar to text100: 0.0000\n",
      "keyword is similar to text101: 0.0000\n",
      "keyword is similar to text102: 0.0000\n",
      "keyword is similar to text103: 0.0000\n",
      "keyword is similar to text104: 0.0000\n",
      "keyword is similar to text105: 0.0000\n",
      "keyword is similar to text106: 0.0000\n",
      "keyword is similar to text107: 0.0000\n",
      "keyword is similar to text108: 0.0000\n",
      "keyword is similar to text109: 0.0000\n",
      "keyword is similar to text110: 0.0000\n",
      "keyword is similar to text111: 0.0000\n",
      "keyword is similar to text112: 0.0000\n",
      "keyword is similar to text113: 0.0000\n",
      "keyword is similar to text114: 0.0000\n",
      "keyword is similar to text115: 0.0000\n",
      "keyword is similar to text116: 0.0000\n",
      "keyword is similar to text117: 0.0000\n",
      "keyword is similar to text118: 0.0000\n",
      "keyword is similar to text119: 0.0000\n",
      "keyword is similar to text120: 0.0000\n",
      "keyword is similar to text121: 0.0000\n",
      "keyword is similar to text122: 0.0000\n",
      "keyword is similar to text123: 0.0000\n",
      "keyword is similar to text124: 0.0000\n",
      "keyword is similar to text125: 0.0000\n",
      "keyword is similar to text126: 0.0000\n",
      "keyword is similar to text127: 0.0000\n",
      "keyword is similar to text128: 0.0000\n",
      "keyword is similar to text129: 0.0000\n",
      "keyword is similar to text130: 0.0000\n",
      "keyword is similar to text131: 0.0000\n",
      "keyword is similar to text132: 0.0000\n",
      "keyword is similar to text133: 0.0000\n",
      "keyword is similar to text134: 0.0000\n",
      "keyword is similar to text135: 0.0000\n",
      "keyword is similar to text136: 0.0000\n",
      "keyword is similar to text137: 0.0000\n",
      "keyword is similar to text138: 0.0000\n",
      "keyword is similar to text139: 0.0000\n",
      "keyword is similar to text140: 0.0000\n",
      "keyword is similar to text141: 0.0000\n",
      "keyword is similar to text142: 0.0000\n",
      "keyword is similar to text143: 0.0000\n",
      "keyword is similar to text144: 0.0000\n",
      "keyword is similar to text145: 0.0000\n",
      "keyword is similar to text146: 0.0000\n",
      "keyword is similar to text147: 0.0000\n",
      "keyword is similar to text148: 0.0000\n",
      "keyword is similar to text149: 0.0000\n",
      "keyword is similar to text150: 0.0000\n",
      "keyword is similar to text151: 0.0000\n",
      "keyword is similar to text152: 0.0000\n",
      "keyword is similar to text153: 0.0000\n",
      "keyword is similar to text154: 0.0000\n",
      "keyword is similar to text155: 0.0000\n",
      "keyword is similar to text156: 0.0000\n",
      "keyword is similar to text157: 0.0000\n",
      "keyword is similar to text158: 0.0000\n",
      "keyword is similar to text159: 0.0000\n",
      "keyword is similar to text160: 0.0000\n",
      "keyword is similar to text161: 0.0000\n",
      "keyword is similar to text162: 0.0000\n",
      "keyword is similar to text163: 0.0000\n",
      "keyword is similar to text164: 0.0000\n",
      "keyword is similar to text165: 0.0000\n",
      "keyword is similar to text166: 0.0000\n",
      "keyword is similar to text167: 0.0000\n",
      "keyword is similar to text168: 0.0000\n",
      "keyword is similar to text169: 0.0000\n",
      "keyword is similar to text170: 0.0000\n",
      "keyword is similar to text171: 0.0000\n",
      "keyword is similar to text172: 0.0000\n",
      "keyword is similar to text173: 0.0000\n",
      "keyword is similar to text174: 0.0000\n",
      "keyword is similar to text175: 0.0000\n",
      "keyword is similar to text176: 0.0000\n",
      "keyword is similar to text177: 0.0000\n",
      "keyword is similar to text178: 0.0000\n",
      "keyword is similar to text179: 0.0000\n",
      "keyword is similar to text180: 0.0000\n",
      "keyword is similar to text181: 0.0000\n",
      "keyword is similar to text182: 0.0000\n",
      "keyword is similar to text183: 0.0000\n",
      "keyword is similar to text184: 0.0000\n",
      "keyword is similar to text185: 0.0000\n",
      "keyword is similar to text186: 0.0000\n",
      "keyword is similar to text187: 0.0000\n",
      "keyword is similar to text188: 0.0000\n",
      "keyword is similar to text189: 0.0000\n",
      "keyword is similar to text190: 0.0000\n",
      "keyword is similar to text191: 0.0000\n",
      "keyword is similar to text192: 0.0000\n",
      "keyword is similar to text193: 0.0000\n",
      "keyword is similar to text194: 0.0000\n",
      "keyword is similar to text195: 0.0000\n",
      "keyword is similar to text196: 0.0000\n",
      "keyword is similar to text197: 0.0000\n",
      "keyword is similar to text198: 0.0000\n",
      "keyword is similar to text199: 0.0000\n",
      "keyword is similar to text200: 0.0000\n",
      "keyword is similar to text201: 0.0000\n",
      "keyword is similar to text202: 0.0000\n",
      "keyword is similar to text203: 0.0000\n",
      "keyword is similar to text204: 0.0000\n",
      "keyword is similar to text205: 0.0000\n",
      "keyword is similar to text206: 0.0000\n",
      "keyword is similar to text207: 0.0000\n",
      "keyword is similar to text208: 0.0000\n",
      "keyword is similar to text209: 0.0000\n",
      "keyword is similar to text210: 0.0000\n",
      "keyword is similar to text211: 0.0000\n",
      "keyword is similar to text212: 0.0000\n",
      "keyword is similar to text213: 0.0000\n",
      "keyword is similar to text214: 0.0000\n",
      "keyword is similar to text215: 0.0000\n",
      "keyword is similar to text216: 0.0000\n",
      "keyword is similar to text217: 0.0000\n",
      "keyword is similar to text218: 0.0000\n",
      "keyword is similar to text219: 0.0000\n",
      "keyword is similar to text220: 0.0000\n",
      "keyword is similar to text221: 0.0000\n",
      "keyword is similar to text222: 0.0000\n",
      "keyword is similar to text223: 0.0000\n",
      "keyword is similar to text224: 0.0000\n",
      "keyword is similar to text225: 0.0000\n",
      "keyword is similar to text226: 0.0000\n",
      "keyword is similar to text227: 0.0000\n",
      "keyword is similar to text228: 0.0000\n",
      "keyword is similar to text229: 0.0000\n",
      "keyword is similar to text230: 0.0000\n",
      "keyword is similar to text231: 0.0000\n",
      "keyword is similar to text232: 0.0000\n",
      "keyword is similar to text233: 0.0000\n",
      "keyword is similar to text234: 0.0000\n",
      "keyword is similar to text235: 0.0000\n",
      "keyword is similar to text236: 0.0000\n",
      "keyword is similar to text237: 0.0000\n",
      "keyword is similar to text238: 0.0000\n",
      "keyword is similar to text239: 0.0000\n",
      "keyword is similar to text240: 0.0000\n",
      "keyword is similar to text241: 0.0000\n",
      "keyword is similar to text242: 0.0000\n",
      "keyword is similar to text243: 0.0000\n",
      "keyword is similar to text244: 0.0000\n",
      "keyword is similar to text245: 0.0000\n",
      "keyword is similar to text246: 0.0000\n",
      "keyword is similar to text247: 0.0000\n",
      "keyword is similar to text248: 0.0000\n",
      "keyword is similar to text249: 0.0000\n",
      "keyword is similar to text250: 0.0000\n",
      "keyword is similar to text251: 0.0000\n",
      "keyword is similar to text252: 0.0000\n",
      "keyword is similar to text253: 0.0000\n",
      "keyword is similar to text254: 0.0000\n",
      "keyword is similar to text255: 0.0000\n",
      "keyword is similar to text256: 0.0000\n",
      "keyword is similar to text257: 0.0000\n",
      "keyword is similar to text258: 0.0000\n",
      "keyword is similar to text259: 0.0000\n",
      "keyword is similar to text260: 0.0000\n",
      "keyword is similar to text261: 0.0000\n",
      "keyword is similar to text262: 0.0000\n",
      "keyword is similar to text263: 0.0000\n",
      "keyword is similar to text264: 0.0000\n",
      "keyword is similar to text265: 0.0000\n",
      "keyword is similar to text266: 0.0000\n",
      "keyword is similar to text267: 0.0000\n",
      "keyword is similar to text268: 0.0000\n",
      "keyword is similar to text269: 0.0000\n",
      "keyword is similar to text270: 0.0000\n",
      "keyword is similar to text271: 0.0000\n",
      "keyword is similar to text272: 0.0000\n",
      "keyword is similar to text273: 0.0000\n",
      "keyword is similar to text274: 0.0000\n",
      "keyword is similar to text275: 0.0000\n",
      "keyword is similar to text276: 0.0000\n",
      "keyword is similar to text277: 0.0000\n",
      "keyword is similar to text278: 0.0000\n",
      "keyword is similar to text279: 0.0000\n",
      "keyword is similar to text280: 0.0000\n",
      "keyword is similar to text281: 0.0000\n",
      "keyword is similar to text282: 0.0000\n",
      "keyword is similar to text283: 0.0000\n",
      "keyword is similar to text284: 0.0000\n",
      "keyword is similar to text285: 0.0000\n",
      "keyword is similar to text286: 0.0000\n",
      "keyword is similar to text287: 0.0000\n",
      "keyword is similar to text288: 0.0000\n",
      "keyword is similar to text289: 0.0000\n",
      "keyword is similar to text290: 0.0000\n",
      "keyword is similar to text291: 0.0000\n",
      "keyword is similar to text292: 0.0000\n",
      "keyword is similar to text293: 0.0000\n",
      "keyword is similar to text294: 0.0000\n",
      "keyword is similar to text295: 0.0000\n",
      "keyword is similar to text296: 0.0000\n",
      "keyword is similar to text297: 0.0000\n",
      "keyword is similar to text298: 0.0000\n",
      "keyword is similar to text299: 0.0000\n",
      "keyword is similar to text300: 0.0000\n",
      "keyword is similar to text301: 0.0000\n",
      "keyword is similar to text302: 0.0000\n",
      "keyword is similar to text303: 0.0000\n",
      "keyword is similar to text304: 0.0000\n",
      "keyword is similar to text305: 0.0000\n",
      "keyword is similar to text306: 0.0000\n",
      "keyword is similar to text307: 0.0000\n",
      "keyword is similar to text308: 0.0000\n",
      "keyword is similar to text309: 0.0000\n",
      "keyword is similar to text310: 0.0000\n",
      "keyword is similar to text311: 0.0000\n",
      "keyword is similar to text312: 0.0000\n",
      "keyword is similar to text313: 0.0000\n",
      "keyword is similar to text314: 0.0000\n",
      "keyword is similar to text315: 0.0000\n",
      "keyword is similar to text316: 0.0000\n",
      "keyword is similar to text317: 0.0000\n",
      "keyword is similar to text318: 0.0000\n",
      "keyword is similar to text319: 0.0000\n",
      "keyword is similar to text320: 0.0000\n",
      "keyword is similar to text321: 0.0000\n",
      "keyword is similar to text322: 0.0000\n",
      "keyword is similar to text323: 0.0000\n",
      "keyword is similar to text324: 0.0000\n",
      "keyword is similar to text325: 0.0000\n",
      "keyword is similar to text326: 0.0000\n",
      "keyword is similar to text327: 0.0000\n",
      "keyword is similar to text328: 0.0000\n",
      "keyword is similar to text329: 0.0000\n",
      "keyword is similar to text330: 0.0000\n",
      "keyword is similar to text331: 0.0000\n",
      "keyword is similar to text332: 0.0000\n",
      "keyword is similar to text333: 0.0000\n",
      "keyword is similar to text334: 0.0000\n",
      "keyword is similar to text335: 0.0000\n",
      "keyword is similar to text336: 0.0000\n",
      "keyword is similar to text337: 0.0000\n",
      "keyword is similar to text338: 0.0000\n",
      "keyword is similar to text339: 0.0000\n",
      "keyword is similar to text340: 0.0000\n",
      "keyword is similar to text341: 0.0000\n",
      "keyword is similar to text342: 0.0000\n",
      "keyword is similar to text343: 0.0000\n",
      "keyword is similar to text344: 0.0000\n",
      "keyword is similar to text345: 0.0000\n",
      "keyword is similar to text346: 0.0000\n",
      "keyword is similar to text347: 0.0000\n",
      "keyword is similar to text348: 0.0000\n",
      "keyword is similar to text349: 0.0000\n",
      "keyword is similar to text350: 0.0000\n",
      "keyword is similar to text351: 0.0000\n",
      "keyword is similar to text352: 0.6454\n",
      "keyword is similar to text353: 0.0000\n",
      "keyword is similar to text354: 0.0000\n",
      "keyword is similar to text355: 0.0000\n",
      "keyword is similar to text356: 0.0000\n",
      "keyword is similar to text357: 0.0000\n",
      "keyword is similar to text358: 0.0000\n",
      "keyword is similar to text359: 0.0000\n",
      "keyword is similar to text360: 0.0000\n",
      "keyword is similar to text361: 0.0000\n",
      "keyword is similar to text362: 0.0000\n",
      "keyword is similar to text363: 0.0000\n",
      "keyword is similar to text364: 0.0000\n",
      "keyword is similar to text365: 0.0000\n",
      "keyword is similar to text366: 0.0000\n",
      "keyword is similar to text367: 0.0000\n",
      "keyword is similar to text368: 0.0000\n",
      "keyword is similar to text369: 0.0000\n",
      "keyword is similar to text370: 0.0000\n",
      "keyword is similar to text371: 0.0000\n",
      "keyword is similar to text372: 0.0000\n",
      "keyword is similar to text373: 0.0000\n",
      "keyword is similar to text374: 0.0000\n",
      "keyword is similar to text375: 0.0000\n",
      "keyword is similar to text376: 0.0000\n",
      "keyword is similar to text377: 0.0000\n",
      "keyword is similar to text378: 0.0000\n",
      "keyword is similar to text379: 0.0000\n",
      "keyword is similar to text380: 0.0000\n",
      "keyword is similar to text381: 0.0000\n",
      "keyword is similar to text382: 0.0000\n",
      "keyword is similar to text383: 0.0000\n",
      "keyword is similar to text384: 0.0000\n",
      "keyword is similar to text385: 0.0000\n",
      "keyword is similar to text386: 0.0000\n",
      "keyword is similar to text387: 0.0000\n",
      "keyword is similar to text388: 0.0000\n",
      "keyword is similar to text389: 0.0000\n",
      "keyword is similar to text390: 0.0000\n",
      "keyword is similar to text391: 0.0000\n",
      "keyword is similar to text392: 0.0000\n",
      "keyword is similar to text393: 0.0000\n",
      "keyword is similar to text394: 0.0000\n",
      "keyword is similar to text395: 0.0000\n",
      "keyword is similar to text396: 0.0000\n",
      "keyword is similar to text397: 0.0000\n",
      "keyword is similar to text398: 0.0000\n",
      "keyword is similar to text399: 0.0000\n",
      "keyword is similar to text400: 0.0000\n"
     ]
    }
   ],
   "source": [
    "kw_vector = dictionary.doc2bow(keyword)\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features = feature_cnt)\n",
    "sim = index[tfidf[kw_vector]]\n",
    "for i in range(len(sim)):\n",
    "    print('keyword is similar to text%d: %.4f' % (i + 1, sim[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword is similar to text1: 0.50\n",
      "keyword is similar to text2: 0.02\n",
      "keyword is similar to text3: 0.00\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import jieba\n",
    "texts = ['I love reading Japanese novels. My favorite Japanese writer is Tanizaki Junichiro.', 'Natsume Soseki is a well-known Japanese novelist and his Kokoro is a masterpiece.', 'American modern poetry is good. ']\n",
    "keyword = 'Japan has some great novelists. Who is your favorite Japanese writer?'\n",
    "texts = [jieba.lcut(text) for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "feature_cnt = len(dictionary.token2id)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus) \n",
    "kw_vector = dictionary.doc2bow(jieba.lcut(keyword))\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features = feature_cnt)\n",
    "sim = index[tfidf[kw_vector]]\n",
    "for i in range(len(sim)):\n",
    "    print('keyword is similar to text%d: %.2f' % (i + 1, sim[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword is similar to text1: 0.50\n",
      "keyword is similar to text2: 0.02\n",
      "keyword is similar to text3: 0.00\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import jieba\n",
    "texts = ['I love reading Japanese novels. My favorite Japanese writer is Tanizaki Junichiro.', 'Natsume Soseki is a well-known Japanese novelist and his Kokoro is a masterpiece.', 'American modern poetry is good. ']\n",
    "keyword = 'Japan has some great novelists. Who is your favorite Japanese writer?'\n",
    "texts = [jieba.lcut(text) for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "feature_cnt = len(dictionary.token2id)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus) \n",
    "kw_vector = dictionary.doc2bow(jieba.lcut(keyword))\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features = feature_cnt)\n",
    "sim = index[tfidf[kw_vector]]\n",
    "for i in range(len(sim)):\n",
    "    print('keyword is similar to text%d: %.2f' % (i + 1, sim[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busca: COMERCIAL CASA DOS FRIOS - USAR LICINIO DIAS\n",
      "\n",
      "Cosine Sentence --- ARES DOS ANDES - EXPORTACAO & IMPORTACAO LTDA\n",
      "\tSimilarity sentence: 0.08362420100070908\n",
      "\tSimilarity sentence: 0.26518576139191\n",
      "\n",
      "Cosine Sentence --- ADEGA DOS TRES IMPORTADORA\n",
      "\tSimilarity sentence: 0.10482848367219183\n",
      "\tSimilarity sentence: 0.223606797749979\n",
      "\n",
      "Cosine Sentence --- BODEGAS DE LOS ANDES COMERCIO DE VINHOS LTDA\n",
      "\tSimilarity sentence: 0.0\n",
      "\tSimilarity sentence: 0.39317854974639244\n",
      "\n",
      "Cosine Sentence --- ALL WINE IMPORTADORA\n",
      "\tSimilarity sentence: 0.0\n",
      "\tSimilarity sentence: 0.09245003270420486\n",
      "\n",
      "Resultado:\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "from nltk import ngrams\n",
    "\n",
    "#Regex para encontrar tokens\n",
    "REGEX_WORD = re.compile(r'\\w+')\n",
    "#Numero de tokens em sequencia\n",
    "N_GRAM_TOKEN = 3\n",
    "\n",
    "#Faz a normalizacao do texto removendo espacos a mais e tirando acentos\n",
    "def text_normalizer(src):\n",
    "    return re.sub('\\s+', ' ',\n",
    "                normalize('NFKD', src)\n",
    "                   .encode('ASCII','ignore')\n",
    "                   .decode('ASCII')\n",
    "           ).lower().strip()\n",
    "\n",
    "#Faz o calculo de similaridade baseada no coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        coef = float(numerator) / denominator\n",
    "        if coef > 1:\n",
    "            coef = 1\n",
    "        return coef\n",
    "\n",
    "#Monta o vetor de frequencia da sentenca\n",
    "def sentence_to_vector(text, use_text_bigram):\n",
    "    words = REGEX_WORD.findall(text)\n",
    "    accumulator = []\n",
    "    for n in range(1,N_GRAM_TOKEN):\n",
    "        gramas = ngrams(words, n)\n",
    "        for grama in gramas:\n",
    "            accumulator.append(str(grama))\n",
    "\n",
    "    if use_text_bigram:\n",
    "        pairs = get_text_bigrams(text)\n",
    "        for pair in pairs:\n",
    "            accumulator.append(pair)\n",
    "\n",
    "    return Counter(accumulator)\n",
    "\n",
    "#Obtem a similaridade entre duas sentencas\n",
    "def get_sentence_similarity(sentence1, sentence2, use_text_bigram=False):\n",
    "    vector1 = sentence_to_vector(text_normalizer(sentence1), use_text_bigram)\n",
    "    vector2 = sentence_to_vector(text_normalizer(sentence2), use_text_bigram)\n",
    "    return cosine_similarity(vector1, vector2)\n",
    "\n",
    "#Metodo de gerar bigramas de uma string\n",
    "def get_text_bigrams(src):\n",
    "    s = src.lower()\n",
    "    return [s[i:i+2] for i in range(len(s) - 1)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    w1 = 'COMERCIAL CASA DOS FRIOS - USAR LICINIO DIAS'\n",
    "    words = [\n",
    "        'ARES DOS ANDES - EXPORTACAO & IMPORTACAO LTDA', \n",
    "        'ADEGA DOS TRES IMPORTADORA', \n",
    "        'BODEGAS DE LOS ANDES COMERCIO DE VINHOS LTDA', \n",
    "        'ALL WINE IMPORTADORA'\n",
    "    ]\n",
    "\n",
    "    print('Busca: ' + w1)\n",
    "\n",
    "    #Nivel de aceite (40%)\n",
    "    cutoff = 0.40\n",
    "    #Senten√ßas similares\n",
    "    result = []\n",
    "\n",
    "    for w2 in words:\n",
    "        print('\\nCosine Sentence --- ' + w2)\n",
    "\n",
    "        #Calculo usando similaridade do coseno com apenas tokens\n",
    "        similarity_sentence = get_sentence_similarity(w1, w2)\n",
    "        print('\\tSimilarity sentence: ' + str(similarity_sentence))\n",
    "\n",
    "        #Calculo usando similaridade do coseno com tokens e com ngramas do texto\n",
    "        similarity_sentence_text_bigram = get_sentence_similarity(w1, w2, use_text_bigram=True)\n",
    "        print('\\tSimilarity sentence: ' + str(similarity_sentence_text_bigram))\n",
    "\n",
    "        if similarity_sentence >= cutoff:\n",
    "            result.append((w2, similarity_sentence))\n",
    "\n",
    "    print('\\nResultado:')\n",
    "    #Exibe resultados\n",
    "    for data in result:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a frequ√™ncia\n",
    "\n",
    "Calcula quantas vezes uma palavra aparece em um texto ou corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos primeiro \n",
    "fdist = nltk.FreqDist(word_tokens)\n",
    "print('\\nContagem do n√∫mero m√°ximo de ocorr√™ncias do token \"',fdist.max(),'\" : ', fdist[fdist.max()])\n",
    "print('\\nN√∫mero total de tokens distintos : ', fdist.N())\n",
    "print('\\nA seguir est√£o os 10 tokens mais comuns')\n",
    "print(fdist.most_common(40))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Stopwords s√£o palavras comuns que normalmente n√£o contribuem para o significado de uma frase e que por isso podem ser ignoradas em processamento PLN. S√£o palavras como \"The\" e \"a\" ((em ingl√™s) ou \"O/A\" e \"Um/Uma\" ((em portugu√™s). \n",
    "\n",
    "Muitos mecanismos de busca filtram estas palavras (stopwords), como forma de economizar espa√ßo em seus √≠ndices de pesquisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que aqui temos menos palavras, pois as stopwords foram removidas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos tamb√©m criar uma lista de caracteres palavras que devem ser ignoradas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que conseguimor limpar ainda mais as palavras a serem trabalhadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging\n",
    "\n",
    "O POS Tagging √© o processo de rotula√ß√£o de elementos textuais - tipicamente palavras e pontua√ß√£o - com o fim de evidenciar \n",
    "a estrutura gramatical de um determinado trecho de texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "tags = pos_tag(word_tokens3)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber o que √© cada c√≥digo, utilize o exemplo abaixo (no caso para VB)\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming √© a t√©cnica de remover sufixos e prefixos de uma palavra, chamada stem. \n",
    "\n",
    "Por exemplo, o stem da palavra cooking √© cook. Um bom algoritmo sabe que \"ing\" √© um sufixo e pode ser removido. \n",
    "\n",
    "Stemming √© muito usado em mecanismos de buscas para indexa√ß√£o de palavras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Cria o Stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "# Para portugu√™s: SnowballStemmer('portuguese')\n",
    "for word in word_tokens3:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que algumas palavras aparece apenas o stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatiza√ß√£o na lingu√≠stica, √© o processo de agrupar as diferentes formas flexionadas de uma palavra para que possam ser analisadas como um √∫nico item.\n",
    "Na lingu√≠stica computacional, a Lemmatiza√ß√£o √© o processo algor√≠tmico de determina√ß√£o do lema para uma determinada palavra. \n",
    "\n",
    "A Lemmatiza√ß√£o est√° intimamente relacionada com o Stemming. \n",
    "\n",
    "A diferen√ßa √© que um stemmer opera em uma √∫nica palavra sem conhecimento do contexto e, portanto, n√£o pode discriminar entre palavras que t√™m diferentes significados, dependendo da parte da fala. No entanto, os stemmers s√£o geralmente mais f√°ceis de implementar e executar mais rapidamente, e a precis√£o reduzida pode n√£o ser importante para algumas aplica√ß√µes.\n",
    "\n",
    "O Stemmning pode gerar palavras geralmente inexistentes, enquanto as lemas s√£o palavras reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Vamos aplicar √†s palavras\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for word in word_tokens3:\n",
    "    print(wordnet_lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja a diferen√ßa do Lemmatization para o Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exerc√≠cio 1\n",
    "\n",
    "## Crie um c√≥digo Python que fa√ßa o seguinte:\n",
    "    \n",
    " 1) Reuna em duas vari√°veis de texto (textoPos, textoNeg) todas as palavras de todos os documentos da categoria 'pos' e 'neg' respectivamente\n",
    "\n",
    " 2) Execute os itens abaixo para textoPos e textoNeg\n",
    " \n",
    " 2.1) Aplique o m√©todo de tokeniza√ß√£o para separar as palavras\n",
    " \n",
    " 2.2) Remova as stopwords e demais palavras que voc√™ julgar necess√°rio para a limpeza do texto\n",
    " \n",
    " 2.3) Aplique a lematiza√ß√£o\n",
    " \n",
    " 2.4) Calcule e imprima a frequ√™ncia das 1000 palavras mais frequentes ap√≥s todo o processamento \n",
    " \n",
    "Salve o Jupyter Notebook com os resultados e entregue pelo iLang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
