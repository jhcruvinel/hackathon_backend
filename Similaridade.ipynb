{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install psycopg2\n",
    "#!pip install sqlalchemy\n",
    "#!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports iniciais\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dados_teste.csv',delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4721.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2818.420545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2219.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>4693.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7108.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9995.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            numero\n",
       "count   400.000000\n",
       "mean   4721.890000\n",
       "std    2818.420545\n",
       "min      24.000000\n",
       "25%    2219.500000\n",
       "50%    4693.500000\n",
       "75%    7108.750000\n",
       "max    9995.000000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nome        400\n",
       "empresa     400\n",
       "endereco    400\n",
       "data1       400\n",
       "data2       400\n",
       "numero      400\n",
       "dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>empresa</th>\n",
       "      <th>endereco</th>\n",
       "      <th>data1</th>\n",
       "      <th>data2</th>\n",
       "      <th>numero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Anjolie Q. Campbell</td>\n",
       "      <td>Luctus Company</td>\n",
       "      <td>P.O. Box 974, 4866 Integer Av.</td>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>2018-11-03</td>\n",
       "      <td>5796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Deanna F. Burnett</td>\n",
       "      <td>Quisque Nonummy Consulting</td>\n",
       "      <td>504-1917 Urna Avenue</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>7320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Cole Z. Travis</td>\n",
       "      <td>Malesuada Fames Ac Associates</td>\n",
       "      <td>P.O. Box 627, 3320 Aliquam, Road</td>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Sonya Y. Mckinney</td>\n",
       "      <td>Mauris Non LLP</td>\n",
       "      <td>Ap #935-8507 Pede Rd.</td>\n",
       "      <td>2018-10-13</td>\n",
       "      <td>2019-12-04</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Logan V. Holmes</td>\n",
       "      <td>Leo Cras Foundation</td>\n",
       "      <td>Ap #235-1628 Consectetuer Rd.</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>2019-02-03</td>\n",
       "      <td>6218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nome                        empresa  \\\n",
       "0  Anjolie Q. Campbell                 Luctus Company   \n",
       "1    Deanna F. Burnett     Quisque Nonummy Consulting   \n",
       "2       Cole Z. Travis  Malesuada Fames Ac Associates   \n",
       "3    Sonya Y. Mckinney                 Mauris Non LLP   \n",
       "4      Logan V. Holmes            Leo Cras Foundation   \n",
       "\n",
       "                           endereco       data1       data2  numero  \n",
       "0    P.O. Box 974, 4866 Integer Av.  2018-10-23  2018-11-03    5796  \n",
       "1              504-1917 Urna Avenue  2018-12-01  2020-06-10    7320  \n",
       "2  P.O. Box 627, 3320 Aliquam, Road  2020-08-15  2020-09-02    7639  \n",
       "3             Ap #935-8507 Pede Rd.  2018-10-13  2019-12-04      82  \n",
       "4     Ap #235-1628 Consectetuer Rd.  2019-07-18  2019-02-03    6218  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "ignore = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']','``',':','http','html','//members']\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Cria o Stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "def preprocess_text(content):\n",
    "    content.translate(str.maketrans('', '', string.punctuation))\n",
    "    word_tokens = nltk.word_tokenize(content)\n",
    "    #print(len(word_tokens),word_tokens)\n",
    "    word_tokens2 = [w.lower() for w in word_tokens if w not in portuguese_stops]\n",
    "    #print(len(word_tokens2),word_tokens2)\n",
    "    word_tokens3 = [w for w in word_tokens2 if w not in ignore]\n",
    "    #print(len(word_tokens3),word_tokens3)\n",
    "    word_tokens4 = [stemmer.stem(w) for w in word_tokens3]\n",
    "    #print(len(word_tokens4),word_tokens4)\n",
    "    #return word_tokens4\n",
    "    return ' '.join(word_tokens4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = 'Teste de sentença para tokenizar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test sentença tokenizar'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nome'] = df['nome'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install sklearn\n",
    "#!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      anjol q. campbel\n",
      "1      deann f. burnett\n",
      "2           col z. trav\n",
      "3      soni y. mckinney\n",
      "4         logan v. holm\n",
      "             ...       \n",
      "395    olga i. thornton\n",
      "396         xen k. stok\n",
      "397     cassidi k. pott\n",
      "398        kyle p. robl\n",
      "399      jarrod n. robl\n",
      "Name: nome, Length: 400, dtype: object\n"
     ]
    }
   ],
   "source": [
    "texts = df['nome']\n",
    "print(nomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busca: COMERCIAL CASA DOS FRIOS - USAR LICINIO DIAS\n",
      "\n",
      "Cosine Sentence --- ARES DOS ANDES - EXPORTACAO & IMPORTACAO LTDA\n",
      "\tSimilarity sentence: 0.08362420100070908\n",
      "\tSimilarity sentence: 0.26518576139191\n",
      "\n",
      "Cosine Sentence --- ADEGA DOS TRES IMPORTADORA\n",
      "\tSimilarity sentence: 0.10482848367219183\n",
      "\tSimilarity sentence: 0.223606797749979\n",
      "\n",
      "Cosine Sentence --- BODEGAS DE LOS ANDES COMERCIO DE VINHOS LTDA\n",
      "\tSimilarity sentence: 0.0\n",
      "\tSimilarity sentence: 0.39317854974639244\n",
      "\n",
      "Cosine Sentence --- ALL WINE IMPORTADORA\n",
      "\tSimilarity sentence: 0.0\n",
      "\tSimilarity sentence: 0.09245003270420486\n",
      "\n",
      "Resultado:\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import re\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "from nltk import ngrams\n",
    "\n",
    "#Regex para encontrar tokens\n",
    "REGEX_WORD = re.compile(r'\\w+')\n",
    "#Numero de tokens em sequencia\n",
    "N_GRAM_TOKEN = 3\n",
    "\n",
    "#Faz a normalizacao do texto removendo espacos a mais e tirando acentos\n",
    "def text_normalizer(src):\n",
    "    return re.sub('\\s+', ' ',\n",
    "                normalize('NFKD', src)\n",
    "                   .encode('ASCII','ignore')\n",
    "                   .decode('ASCII')\n",
    "           ).lower().strip()\n",
    "\n",
    "#Faz o calculo de similaridade baseada no coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        coef = float(numerator) / denominator\n",
    "        if coef > 1:\n",
    "            coef = 1\n",
    "        return coef\n",
    "\n",
    "#Monta o vetor de frequencia da sentenca\n",
    "def sentence_to_vector(text, use_text_bigram):\n",
    "    words = REGEX_WORD.findall(text)\n",
    "    accumulator = []\n",
    "    for n in range(1,N_GRAM_TOKEN):\n",
    "        gramas = ngrams(words, n)\n",
    "        for grama in gramas:\n",
    "            accumulator.append(str(grama))\n",
    "\n",
    "    if use_text_bigram:\n",
    "        pairs = get_text_bigrams(text)\n",
    "        for pair in pairs:\n",
    "            accumulator.append(pair)\n",
    "\n",
    "    return Counter(accumulator)\n",
    "\n",
    "#Obtem a similaridade entre duas sentencas\n",
    "def get_sentence_similarity(sentence1, sentence2, use_text_bigram=False):\n",
    "    vector1 = sentence_to_vector(text_normalizer(sentence1), use_text_bigram)\n",
    "    vector2 = sentence_to_vector(text_normalizer(sentence2), use_text_bigram)\n",
    "    return cosine_similarity(vector1, vector2)\n",
    "\n",
    "#Metodo de gerar bigramas de uma string\n",
    "def get_text_bigrams(src):\n",
    "    s = src.lower()\n",
    "    return [s[i:i+2] for i in range(len(s) - 1)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    w1 = 'COMERCIAL CASA DOS FRIOS - USAR LICINIO DIAS'\n",
    "    words = [\n",
    "        'ARES DOS ANDES - EXPORTACAO & IMPORTACAO LTDA', \n",
    "        'ADEGA DOS TRES IMPORTADORA', \n",
    "        'BODEGAS DE LOS ANDES COMERCIO DE VINHOS LTDA', \n",
    "        'ALL WINE IMPORTADORA'\n",
    "    ]\n",
    "\n",
    "    print('Busca: ' + w1)\n",
    "\n",
    "    #Nivel de aceite (40%)\n",
    "    cutoff = 0.40\n",
    "    #Sentenças similares\n",
    "    result = []\n",
    "\n",
    "    for w2 in words:\n",
    "        print('\\nCosine Sentence --- ' + w2)\n",
    "\n",
    "        #Calculo usando similaridade do coseno com apenas tokens\n",
    "        similarity_sentence = get_sentence_similarity(w1, w2)\n",
    "        print('\\tSimilarity sentence: ' + str(similarity_sentence))\n",
    "\n",
    "        #Calculo usando similaridade do coseno com tokens e com ngramas do texto\n",
    "        similarity_sentence_text_bigram = get_sentence_similarity(w1, w2, use_text_bigram=True)\n",
    "        print('\\tSimilarity sentence: ' + str(similarity_sentence_text_bigram))\n",
    "\n",
    "        if similarity_sentence >= cutoff:\n",
    "            result.append((w2, similarity_sentence))\n",
    "\n",
    "    print('\\nResultado:')\n",
    "    #Exibe resultados\n",
    "    for data in result:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05423261445466404 anjol q. campbel\n",
      "0.058823529411764705 logan v. holm\n",
      "0.06262242910851494 walt d. pac\n",
      "0.1143323900950059 joshu i. oneil\n",
      "0.11764705882352941 ivan i. buckn\n",
      "0.12126781251816648 ivan f. hobb\n",
      "0.15512630699850574 lillian h. dixon\n",
      "0.16269784336399212 alan x. dalton\n",
      "0.16692446522239712 jason w. daniel\n",
      "0.17647058823529413 elain y savag\n",
      "0.18786728732554484 sil q. vang\n",
      "0.21693045781865616 josiah f. dawson\n",
      "Mais similar a John da silva = josiah f. dawson\n"
     ]
    }
   ],
   "source": [
    "max_sim = 0\n",
    "most_similar = ''\n",
    "test = 'John da silva'\n",
    "for w in texts:\n",
    "    sim = get_sentence_similarity(test, w, use_text_bigram=True)\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        most_similar = w\n",
    "        print(str(sim)+' '+w)\n",
    "print ('Mais similar a ' + test + ' = ' + most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/c6/786987e38465b63ef492b4b7b05ccceaf0413dc1f49127313da243779823/spacy-2.1.8-cp36-cp36m-win_amd64.whl (30.0MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from spacy) (1.17.2)\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/c0/3c238c146ed72f92a1cc3771fa3f66091b80d2a8e15b7c987912443b7843/preshed-2.0.1-cp36-cp36m-win_amd64.whl (73kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Collecting srsly<1.1.0,>=0.0.6 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e4/11/664f4cf38222cfb718724576c16f806f131ef4ad94bf7d8fb9802ecb4a41/srsly-0.1.0-cp36-cp36m-win_amd64.whl (171kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/93/4b543adf6c0d73ed4e05d92abfb644c2743cd656adc8058510fdfac80680/cymem-2.0.2-cp36-cp36m-win_amd64.whl\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting thinc<7.1.0,>=7.0.8 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/36/e3b2463898dc459201ff3c46dfb7add0f7f300e4bcc7f5823552f7531c80/thinc-7.0.8-cp36-cp36m-win_amd64.whl (1.9MB)\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/84/0e/35f8ce00fc3412aa3a888a3d8e040c4c65ccbad8310aa2981d3e6f379867/blis-0.2.4-cp36-cp36m-win_amd64.whl (3.1MB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/15/8f/0dad3ca706e31258cf7b9adf40f8d2103444a09dd7d66d46cf6980025c65/murmurhash-1.0.2-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.3)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<7.1.0,>=7.0.8->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/88/d3213e2f3492daf09d8b41631ad6899f56db17ce83ea9c8a579902bafe5e/tqdm-4.35.0-py2.py3-none-any.whl (50kB)\n",
      "Installing collected packages: cymem, preshed, srsly, plac, wasabi, tqdm, blis, murmurhash, thinc, spacy\n",
      "Successfully installed blis-0.2.4 cymem-2.0.2 murmurhash-1.0.2 plac-0.9.6 preshed-2.0.1 spacy-2.1.8 srsly-0.1.0 thinc-7.0.8 tqdm-4.35.0 wasabi-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'pt'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-1d6ac6d7e903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\hackathon\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\hackathon\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'pt'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05423261445466404 anjol q. campbel\n",
      "0.058823529411764705 logan v. holm\n",
      "0.06262242910851494 walt d. pac\n",
      "0.1143323900950059 joshu i. oneil\n",
      "0.11764705882352941 ivan i. buckn\n",
      "0.12126781251816648 ivan f. hobb\n",
      "0.15512630699850574 lillian h. dixon\n",
      "0.16269784336399212 alan x. dalton\n",
      "0.16692446522239712 jason w. daniel\n",
      "0.17647058823529413 elain y savag\n",
      "0.18786728732554484 sil q. vang\n",
      "0.21693045781865616 josiah f. dawson\n",
      "Mais similar a John da silva = josiah f. dawson\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt')\n",
    " \n",
    "doc1 = nlp(u'Hello this is document similarity calculation')\n",
    "doc2 = nlp(u'Hello this is python similarity calculation')\n",
    "doc3 = nlp(u'Hi there')\n",
    " \n",
    "print (doc1.similarity(doc2)) \n",
    "print (doc2.similarity(doc3)) \n",
    "print (doc1.similarity(doc3))  \n",
    " \n",
    "max_sim = 0\n",
    "most_similar = ''\n",
    "test = 'John da silva'\n",
    "for w in texts:\n",
    "    sim = get_sentence_similarity(test, w, use_text_bigram=True)\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        most_similar = w\n",
    "        print(str(sim)+' '+w)\n",
    "print ('Mais similar a ' + test + ' = ' + most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: Jinja2>=2.4 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from flask) (2.10.1)\n",
      "Requirement already satisfied: itsdangerous>=0.21 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from flask) (1.1.0)\n",
      "Requirement already satisfied: click>=2.0 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from flask) (7.0)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from flask) (0.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jhcru\\appdata\\local\\conda\\conda\\envs\\hackathon\\lib\\site-packages (from Jinja2>=2.4->flask) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
