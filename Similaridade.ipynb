{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install psycopg2\n",
    "#!pip install sqlalchemy\n",
    "#!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacoes\n",
    "import os\n",
    "import ast\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "from nltk import ngrams\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "from flask_cors import CORS\n",
    "from nltk.stem import SnowballStemmer\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Constantes\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "ignore = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']','``',':','http','html','//members']\n",
    "stemmer = SnowballStemmer('english')\n",
    "# Regex para encontrar tokens\n",
    "REGEX_WORD = re.compile(r'\\w+')\n",
    "# Numero de tokens em sequencia\n",
    "N_GRAM_TOKEN = 3\n",
    "DATABASE = 'banco.csv'\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# FUNÇOES DE PRE-PROCESSAMENTO DE TEXTO\n",
    "\n",
    "# Funcao de pre-processamento do texto\n",
    "def preprocess_text(content):\n",
    "    # Remove pontuacoes\n",
    "    content.translate(str.maketrans('', '', string.punctuation))\n",
    "    word_tokens = nltk.word_tokenize(content)\n",
    "    # Remove stopwords e coloca em minusculo\n",
    "    word_tokens2 = [w.lower() for w in word_tokens if w not in portuguese_stops]\n",
    "    # remove lista de ignore\n",
    "    word_tokens3 = [w for w in word_tokens2 if w not in ignore]\n",
    "    # aplica o stemming\n",
    "    word_tokens4 = [stemmer.stem(w) for w in word_tokens3]\n",
    "    #return word_tokens4\n",
    "    return ' '.join(word_tokens4)\n",
    "\n",
    "# Faz a normalizacao do texto removendo espacos a mais e tirando acentos\n",
    "def text_normalizer(src):\n",
    "    return re.sub('\\s+', ' ',\n",
    "                normalize('NFKD', src)\n",
    "                   .encode('ASCII','ignore')\n",
    "                   .decode('ASCII')\n",
    "           ).lower().strip()\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# FUNÇOES DE CALCULO DE SIMILARIDADE\n",
    "\n",
    "# Calculo de similaridade do coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        coef = float(numerator) / denominator\n",
    "        if coef > 1:\n",
    "            coef = 1\n",
    "        return coef\n",
    "\n",
    "# Monta o vetor de frequencia da sentenca\n",
    "def sentence_to_vector(text, use_text_bigram):\n",
    "    words = REGEX_WORD.findall(text)\n",
    "    accumulator = []\n",
    "    for n in range(1,N_GRAM_TOKEN):\n",
    "        gramas = ngrams(words, n)\n",
    "        for grama in gramas:\n",
    "            accumulator.append(str(grama))\n",
    "    if use_text_bigram:\n",
    "        pairs = get_text_bigrams(text)\n",
    "        for pair in pairs:\n",
    "            accumulator.append(pair)\n",
    "    return Counter(accumulator)\n",
    "\n",
    "# Calcula similaridade entre duas sentencas\n",
    "def get_sentence_similarity(sentence1, sentence2, use_text_bigram=False):\n",
    "    vector1 = sentence_to_vector(text_normalizer(sentence1), use_text_bigram)\n",
    "    vector2 = sentence_to_vector(text_normalizer(sentence2), use_text_bigram)\n",
    "    return cosine_similarity(vector1, vector2)\n",
    "\n",
    "# Metodo de gerar bigramas de uma string\n",
    "def get_text_bigrams(src):\n",
    "    s = src.lower()\n",
    "    return [s[i:i+2] for i in range(len(s) - 1)]\n",
    "\n",
    "# Simulando o banco\n",
    "df = pd.read_csv('dados_teste.csv',delimiter='|')\n",
    "df['nome'] = df['nome'].apply(preprocess_text)\n",
    "texts = df['nome']\n",
    "\n",
    "def most_similar(sentences,sentence):\n",
    "    max_sim = 0\n",
    "    most_similar = ''\n",
    "    for s in sentences:\n",
    "        sim = get_sentence_similarity(sentence, s, use_text_bigram=True)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            most_similar = s\n",
    "            print(str(sim)+' '+s)\n",
    "    print ('Mais similar a ' + sentence + ' = ' + most_similar + ' with distance = ' + str(max_sim))\n",
    "    return {'doc':most_similar}\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# FUNÇOES PARA SIMULACAO DE BANCO DE DADOS COM PANDAS\n",
    "\n",
    "# Salva dados no banco\n",
    "def write_to_csv_file_by_pandas(data_frame):\n",
    "    data_frame.to_csv(DATABASE, index=False)\n",
    "    print(DATABASE + ' has been created.')\n",
    "\n",
    "# Leitura \n",
    "def read_csv_file_by_pandas():\n",
    "    data_frame = None\n",
    "    if(os.path.exists(DATABASE)):\n",
    "        data_frame = pd.read_csv(DATABASE, index_col=False)\n",
    "    else:\n",
    "        print(DATABASE + \" do not exist.\")    \n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = 'banco.csv'\n",
    "df_existente = pd.read_csv(DATABASE,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo para concatenar em uma string mais de um pedido para fins de comparacao\n",
    "def concatenar_pedidos(pedidos):\n",
    "    pedido_concatenado = ''\n",
    "    for pedido in pedidos:\n",
    "        pedido_concatenado += pedido['tipo']+\"_\"\n",
    "    return pedido_concatenado\n",
    "\n",
    "# Metodo para o calculo ponderado da diferenca entre processos\n",
    "def calcula_diferenca_processos(p1, p2):\n",
    "    dif = {}\n",
    "    print(p1)\n",
    "    print(p2)\n",
    "    # So compara se nao for com si mesmo\n",
    "    if (p1['id'] != p2['id'].iloc[0]):\n",
    "        # Comparacao dos Nomes das Partes Reclamadas\n",
    "        nomes_reclamante_parecidos = get_sentence_similarity(p1['nomeParteReclamante'] , p2['nomeParteReclamante'].iloc[0], use_text_bigram=True)\n",
    "        dif['nomes_reclamante_parecidos'] = nomes_reclamante_parecidos\n",
    "        # Comparacao dos Nomes das Partes Reclamantes\n",
    "        nomes_reclamada_parecidos = get_sentence_similarity(p1['nomeParteReclamada'] , p2['nomeParteReclamada'].iloc[0], use_text_bigram=True)\n",
    "        dif['nomes_reclamada_parecidos'] = nomes_reclamada_parecidos\n",
    "        # Compracao do prazo de Ajuizamento\n",
    "        d1 = datetime.strptime(p1['dataAjuizamentoInicial'], '%Y-%m-%d').date()\n",
    "        d2 = datetime.strptime(p2['dataAjuizamentoInicial'].iloc[0], '%Y-%m-%d').date()\n",
    "        dif_dataAjuizamentoInicial = abs((d2 - d1).days)\n",
    "        dif['dif_dataAjuizamentoInicial'] = dif_dataAjuizamentoInicial\n",
    "        # Comparacao dos meses de contratacao\n",
    "        d1 = datetime.strptime(p1['dataInicioContrato'], '%Y-%m-%d').date()\n",
    "        d2 = datetime.strptime(p2['dataTerminoContrato'].iloc[0], '%Y-%m-%d').date()\n",
    "        dif_meses = abs((d2 - d1).days)/30\n",
    "        dif['dif_meses'] = dif_meses\n",
    "        # Comparacao dos meses de salario proporcional de 13\n",
    "        propMeses13P1 = float(p1['meses13SalarioProporcional'])\n",
    "        propMeses13P2 = float(p2['meses13SalarioProporcional'].iloc[0])\n",
    "        dif_propMeses13P = propMeses13P1-propMeses13P2\n",
    "        dif['dif_propMeses13P'] = dif_propMeses13P\n",
    "        # Comparacao dos salarios\n",
    "        salario1 = float(p1['salario'])\n",
    "        salario2 = float(p2['salario'].iloc[0])\n",
    "        dif_salario = salario1-salario2\n",
    "        dif['dif_salario'] = dif_salario\n",
    "        # Comparacao das jornadas\n",
    "        jornadaSemanal1 = int(p1['jornadaSemanal'])\n",
    "        jornadaSemanal2 = int(p2['jornadaSemanal'].iloc[0])\n",
    "        dif_jornadaSemanal = jornadaSemanal1-jornadaSemanal2\n",
    "        dif['dif_jornadaSemanal'] = dif_jornadaSemanal\n",
    "        # Comparacao dos pedidos\n",
    "        pedido_concatenado1 = concatenar_pedidos(ast.literal_eval(p1['pedidos']))\n",
    "        pedido_concatenado2 = concatenar_pedidos(ast.literal_eval(p2['pedidos'].iloc[0]))\n",
    "        pedidos_iguais = pedido_concatenado1 == pedido_concatenado2\n",
    "        dif['pedidos_iguais'] = pedidos_iguais\n",
    "        # Calculo da distancia total entre pedidos com uso de pesos diferenciados\n",
    "        dif_total = nomes_reclamante_parecidos * 1.5\n",
    "        dif_total += nomes_reclamada_parecidos * 1.5\n",
    "        dif_total += dif_dataAjuizamentoInicial / 10\n",
    "        dif_total += dif_meses / 48\n",
    "        dif_total += dif_propMeses13P / 6\n",
    "        dif_total += dif_salario / 2000\n",
    "        dif_total += dif_jornadaSemanal / 44\n",
    "        dif['dif_total'] = dif_total\n",
    "        print(dif)\n",
    "    return dif\n",
    "\n",
    "def calcula_probabilidade_acordo(df_processo, df_existente):\n",
    "    for i, p in df_existente.iterrows():\n",
    "        dif = calcula_diferenca_processos(p,df_processo)\n",
    "        print(dif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando Insight para Processo ID: 1\n",
      "recuperou\n",
      "{}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 2, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 1300.0, 'dif_jornadaSemanal': -19, 'pedidos_iguais': True, 'dif_total': 2.800820707070707}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 2, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 1300.0, 'dif_jornadaSemanal': -19, 'pedidos_iguais': True, 'dif_total': 2.800820707070707}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 2, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 1000.0, 'dif_jornadaSemanal': -8, 'pedidos_iguais': True, 'dif_total': 2.9008207070707073}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 2, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 1000.0, 'dif_jornadaSemanal': -8, 'pedidos_iguais': True, 'dif_total': 2.9008207070707073}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 2, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 1300.0, 'dif_jornadaSemanal': -3, 'pedidos_iguais': True, 'dif_total': 3.1644570707070705}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 2, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 1300.0, 'dif_jornadaSemanal': -3, 'pedidos_iguais': True, 'dif_total': 3.1644570707070705}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 0, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 3300.0, 'dif_jornadaSemanal': 0, 'pedidos_iguais': True, 'dif_total': 4.032638888888888}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 0, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 3300.0, 'dif_jornadaSemanal': 0, 'pedidos_iguais': True, 'dif_total': 4.032638888888888}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 0, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 10300.0, 'dif_jornadaSemanal': -20, 'pedidos_iguais': True, 'dif_total': 7.078093434343435}\n",
      "{'nomes_reclamante_parecidos': True, 'nomes_reclamada_parecidos': True, 'dif_dataAjuizamentoInicial': 0, 'dif_meses': 18.366666666666667, 'dif_propMeses13P': 0.0, 'dif_salario': 10300.0, 'dif_jornadaSemanal': -20, 'pedidos_iguais': True, 'dif_total': 7.078093434343435}\n"
     ]
    }
   ],
   "source": [
    "id = 1\n",
    "print (\"Buscando Insight para Processo ID: \"+str(id))\n",
    "df_existente = pd.read_csv(DATABASE,header=0)\n",
    "df_processo = df_existente.loc[df_existente['id'] == id]\n",
    "print('recuperou')\n",
    "calcula_probabilidade_acordo(df_processo, df_existente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-81f90d5a013c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_existente\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mcalcula_diferenca_processos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_processo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-81f90d5a013c>\u001b[0m in \u001b[0;36mcalcula_diferenca_processos\u001b[1;34m(p1, p2)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdif\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# So compara se nao for com si mesmo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Comparacao dos Nomes das Partes Reclamadas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mnomes_reclamante_parecidos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nomeParteReclamante'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nomeParteReclamante'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\hackathon\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1426\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\hackathon\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2157\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2159\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\hackathon\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2086\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2087\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2088\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2090\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from datetime import datetime\n",
    "# Metodo para concatenar em uma string mais de um pedido para fins de comparacao\n",
    "def concatenar_pedidos(pedidos):\n",
    "    pedido_concatenado = ''\n",
    "    for pedido in pedidos:\n",
    "        pedido_concatenado += pedido['tipo']+\"_\"\n",
    "    return pedido_concatenado\n",
    "\n",
    "def calcula_diferenca_processos(p1, p2):\n",
    "    dif = {}\n",
    "    # So compara se nao for com si mesmo\n",
    "    if (p1['id'] != p2['id'].iloc[0]):\n",
    "        # Comparacao dos Nomes das Partes Reclamadas\n",
    "        nomes_reclamante_parecidos = p1['nomeParteReclamante'] == p2['nomeParteReclamante'].iloc[0]\n",
    "        dif['nomes_reclamante_parecidos'] = nomes_reclamante_parecidos\n",
    "        # Comparacao dos Nomes das Partes Reclamantes\n",
    "        nomes_reclamada_parecidos = p1['nomeParteReclamada'] == p2['nomeParteReclamada'].iloc[0]\n",
    "        dif['nomes_reclamada_parecidos'] = nomes_reclamada_parecidos\n",
    "        # Comapracao do prazo de Ajuizamento\n",
    "        d1 = datetime.strptime(p1['dataAjuizamentoInicial'], '%Y-%m-%d').date()\n",
    "        d2 = datetime.strptime(p2['dataAjuizamentoInicial'].iloc[0], '%Y-%m-%d').date()\n",
    "        dif_dataAjuizamentoInicial = abs((d2 - d1).days)\n",
    "        dif['dif_dataAjuizamentoInicial'] = dif_dataAjuizamentoInicial\n",
    "        # Calculo de dias de contratacao\n",
    "        d1 = datetime.strptime(p1['dataInicioContrato'], '%Y-%m-%d').date()\n",
    "        d2 = datetime.strptime(p2['dataTerminoContrato'].iloc[0], '%Y-%m-%d').date()\n",
    "        dif_meses = abs((d2 - d1).days)/30\n",
    "        dif['dif_meses'] = dif_meses\n",
    "        propMeses13P1 = float(p1['meses13SalarioProporcional'])\n",
    "        propMeses13P2 = float(p2['meses13SalarioProporcional'].iloc[0])\n",
    "        dif_propMeses13P = propMeses13P1-propMeses13P2\n",
    "        dif['dif_propMeses13P'] = dif_propMeses13P\n",
    "        salario1 = float(p1['salario'])\n",
    "        salario2 = float(p2['salario'].iloc[0])\n",
    "        dif_salario = salario1-salario2\n",
    "        dif['dif_salario'] = dif_salario\n",
    "        jornadaSemanal1 = int(p1['jornadaSemanal'])\n",
    "        jornadaSemanal2 = int(p2['jornadaSemanal'].iloc[0])\n",
    "        dif_jornadaSemanal = jornadaSemanal1-jornadaSemanal2\n",
    "        dif['dif_jornadaSemanal'] = dif_jornadaSemanal\n",
    "        pedido_concatenado1 = concatenar_pedidos(ast.literal_eval(p1['pedidos']))\n",
    "        pedido_concatenado2 = concatenar_pedidos(ast.literal_eval(p2['pedidos'].iloc[0]))\n",
    "        pedidos_iguais = pedido_concatenado1 == pedido_concatenado2\n",
    "        dif['pedidos_iguais'] = pedidos_iguais\n",
    "        dif_total = 1 if nomes_reclamante_parecidos == True else 0\n",
    "        dif_total += 1 if nomes_reclamada_parecidos == True else 0\n",
    "        dif_total += dif_dataAjuizamentoInicial/10\n",
    "        dif_total += dif_meses/48\n",
    "        dif_total += dif_propMeses13P/6\n",
    "        dif_total += dif_salario/2000\n",
    "        dif_total += dif_jornadaSemanal/44\n",
    "        dif['dif_total'] = dif_total\n",
    "        print (dif)\n",
    "    return dif\n",
    "\n",
    "for i, p in df_existente.iterrows():\n",
    "    calcula_diferenca_processos(p,df_processo)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dados_teste.csv',delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "ignore = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']','``',':','http','html','//members']\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Cria o Stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "def preprocess_text(content):\n",
    "    content.translate(str.maketrans('', '', string.punctuation))\n",
    "    word_tokens = nltk.word_tokenize(content)\n",
    "    #print(len(word_tokens),word_tokens)\n",
    "    word_tokens2 = [w.lower() for w in word_tokens if w not in portuguese_stops]\n",
    "    #print(len(word_tokens2),word_tokens2)\n",
    "    word_tokens3 = [w for w in word_tokens2 if w not in ignore]\n",
    "    #print(len(word_tokens3),word_tokens3)\n",
    "    word_tokens4 = [stemmer.stem(w) for w in word_tokens3]\n",
    "    #print(len(word_tokens4),word_tokens4)\n",
    "    #return word_tokens4\n",
    "    return ' '.join(word_tokens4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = 'Teste de sentença para tokenizar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nome'] = df['nome'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install sklearn\n",
    "#!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['nome']\n",
    "print(nomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "from nltk import ngrams\n",
    "\n",
    "#Regex para encontrar tokens\n",
    "REGEX_WORD = re.compile(r'\\w+')\n",
    "#Numero de tokens em sequencia\n",
    "N_GRAM_TOKEN = 3\n",
    "\n",
    "#Faz a normalizacao do texto removendo espacos a mais e tirando acentos\n",
    "def text_normalizer(src):\n",
    "    return re.sub('\\s+', ' ',\n",
    "                normalize('NFKD', src)\n",
    "                   .encode('ASCII','ignore')\n",
    "                   .decode('ASCII')\n",
    "           ).lower().strip()\n",
    "\n",
    "#Faz o calculo de similaridade baseada no coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        coef = float(numerator) / denominator\n",
    "        if coef > 1:\n",
    "            coef = 1\n",
    "        return coef\n",
    "\n",
    "#Monta o vetor de frequencia da sentenca\n",
    "def sentence_to_vector(text, use_text_bigram):\n",
    "    words = REGEX_WORD.findall(text)\n",
    "    accumulator = []\n",
    "    for n in range(1,N_GRAM_TOKEN):\n",
    "        gramas = ngrams(words, n)\n",
    "        for grama in gramas:\n",
    "            accumulator.append(str(grama))\n",
    "\n",
    "    if use_text_bigram:\n",
    "        pairs = get_text_bigrams(text)\n",
    "        for pair in pairs:\n",
    "            accumulator.append(pair)\n",
    "\n",
    "    return Counter(accumulator)\n",
    "\n",
    "#Obtem a similaridade entre duas sentencas\n",
    "def get_sentence_similarity(sentence1, sentence2, use_text_bigram=False):\n",
    "    vector1 = sentence_to_vector(text_normalizer(sentence1), use_text_bigram)\n",
    "    vector2 = sentence_to_vector(text_normalizer(sentence2), use_text_bigram)\n",
    "    return cosine_similarity(vector1, vector2)\n",
    "\n",
    "#Metodo de gerar bigramas de uma string\n",
    "def get_text_bigrams(src):\n",
    "    s = src.lower()\n",
    "    return [s[i:i+2] for i in range(len(s) - 1)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    w1 = 'COMERCIAL CASA DOS FRIOS - USAR LICINIO DIAS'\n",
    "    words = [\n",
    "        'ARES DOS ANDES - EXPORTACAO & IMPORTACAO LTDA', \n",
    "        'ADEGA DOS TRES IMPORTADORA', \n",
    "        'BODEGAS DE LOS ANDES COMERCIO DE VINHOS LTDA', \n",
    "        'ALL WINE IMPORTADORA'\n",
    "    ]\n",
    "\n",
    "    print('Busca: ' + w1)\n",
    "\n",
    "    #Nivel de aceite (40%)\n",
    "    cutoff = 0.40\n",
    "    #Sentenças similares\n",
    "    result = []\n",
    "\n",
    "    for w2 in words:\n",
    "        print('\\nCosine Sentence --- ' + w2)\n",
    "\n",
    "        #Calculo usando similaridade do coseno com apenas tokens\n",
    "        similarity_sentence = get_sentence_similarity(w1, w2)\n",
    "        print('\\tSimilarity sentence: ' + str(similarity_sentence))\n",
    "\n",
    "        #Calculo usando similaridade do coseno com tokens e com ngramas do texto\n",
    "        similarity_sentence_text_bigram = get_sentence_similarity(w1, w2, use_text_bigram=True)\n",
    "        print('\\tSimilarity sentence: ' + str(similarity_sentence_text_bigram))\n",
    "\n",
    "        if similarity_sentence >= cutoff:\n",
    "            result.append((w2, similarity_sentence))\n",
    "\n",
    "    print('\\nResultado:')\n",
    "    #Exibe resultados\n",
    "    for data in result:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sim = 0\n",
    "most_similar = ''\n",
    "test = 'John da silva'\n",
    "for w in texts:\n",
    "    sim = get_sentence_similarity(test, w, use_text_bigram=True)\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        most_similar = w\n",
    "        print(str(sim)+' '+w)\n",
    "print ('Mais similar a ' + test + ' = ' + most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt')\n",
    " \n",
    "doc1 = nlp(u'Hello this is document similarity calculation')\n",
    "doc2 = nlp(u'Hello this is python similarity calculation')\n",
    "doc3 = nlp(u'Hi there')\n",
    " \n",
    "print (doc1.similarity(doc2)) \n",
    "print (doc2.similarity(doc3)) \n",
    "print (doc1.similarity(doc3))  \n",
    " \n",
    "max_sim = 0\n",
    "most_similar = ''\n",
    "test = 'John da silva'\n",
    "for w in texts:\n",
    "    sim = get_sentence_similarity(test, w, use_text_bigram=True)\n",
    "    if sim > max_sim:\n",
    "        max_sim = sim\n",
    "        most_similar = w\n",
    "        print(str(sim)+' '+w)\n",
    "print ('Mais similar a ' + test + ' = ' + most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedido1 = {\"descricaoCadastroPartes\":null,\"nomeParteReclamante\":\"José Reclamante da Silva\",\"cpfParteReclamante\":\"281.690.010-05\",\"cpfCnpjParteReclamada\":\"26.965.944/0001-97\",\"dataAjuizamentoInicial\":\"2019-09-13\",\"dataInicioContrato\":\"2014-03-20\",\"feriasVencidas\":null,\"decimoTerceiroVencido\":null,\"mesesFgtsNaoDepositado\":null,\"multaRescisoriaFgtsDepositado\":0,\"nomeParteReclamada\":\"Companhia Fictícia\",\"dataTerminoContrato\":\"2015-04-22\",\"salario\":1788.38,\"diasAvisoPrevio\":13,\"meses13SalarioProporcional\":0.5,\"existenciaVinculoEmpregaticio\":null,\"carteiraDevidamenteAnotada\":null,\"funcao\":\"Estivador\",\"motivoRescisao\":null,\"jornadaSemanal\":44,\"justicaGratuita\":null,\"pedidos\":[{\"tipo\":\"AVISO_PREVIO\",\"valor_pedido\":13},{\"tipo\":\"DECIMO_TERC_PROPORCIONAL\",\"valor_pedido\":0.5}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
