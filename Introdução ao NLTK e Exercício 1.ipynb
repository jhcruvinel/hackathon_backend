{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neste lab vamos conhecer um pouco sobre o NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar um dataset de revisão de filmes disponível em http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "Para esta prática baixamos o dataset http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip\n",
    "\n",
    "Baixe e descompacte este arquivo zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiramente vamos abrir e ler o conteúdo de um dos arquivos baixados\n",
    "import os\n",
    "import nltk\n",
    "with open(os.path.join(\"tokens\",\"neg\",  \"cv011_tok-7845.txt\"), \"r\") as f:\n",
    "    text = f.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização\n",
    "\n",
    "Processo de dividir uma string em listas de pedaços ou \"tokens\". \n",
    "Um token é uma parte inteira. Por exemplo: uma palavra é um token em uma sentença. Uma sentença é um token em um parágrafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização de sentenças\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent_tokens = sent_tokenize(text)\n",
    "for sentence in sent_tokens:\n",
    "    print (' - ',sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização de palavras\n",
    "word_tokens = word_tokenize(text)\n",
    "print(len(word_tokens),word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a frequência\n",
    "\n",
    "Calcula quantas vezes uma palavra aparece em um texto ou corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos primeiro \n",
    "fdist = nltk.FreqDist(word_tokens)\n",
    "print('\\nContagem do número máximo de ocorrências do token \"',fdist.max(),'\" : ', fdist[fdist.max()])\n",
    "print('\\nNúmero total de tokens distintos : ', fdist.N())\n",
    "print('\\nA seguir estão os 10 tokens mais comuns')\n",
    "print(fdist.most_common(40))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Stopwords são palavras comuns que normalmente não contribuem para o significado de uma frase e que por isso podem ser ignoradas em processamento PLN. São palavras como \"The\" e \"a\" ((em inglês) ou \"O/A\" e \"Um/Uma\" ((em português). \n",
    "\n",
    "Muitos mecanismos de busca filtram estas palavras (stopwords), como forma de economizar espaço em seus índices de pesquisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Stop words em inglês\n",
    "english_stops = set(stopwords.words('english'))\n",
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words em português\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "print(portuguese_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension para aplicar as portuguese_stop words a lista de palavras\n",
    "word_tokens2 = [w for w in word_tokens if w not in english_stops]\n",
    "print(len(word_tokens2),word_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que aqui temos menos palavras, pois as stopwords foram removidas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos também criar uma lista de caracteres palavras que devem ser ignoradas\n",
    "ignore = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']','``',':','http','html','//members']\n",
    "word_tokens3 = [w for w in word_tokens2 if w not in ignore]\n",
    "print(len(word_tokens3),word_tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que conseguimor limpar ainda mais as palavras a serem trabalhadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging\n",
    "\n",
    "O POS Tagging é o processo de rotulação de elementos textuais - tipicamente palavras e pontuação - com o fim de evidenciar \n",
    "a estrutura gramatical de um determinado trecho de texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "tags = pos_tag(word_tokens3)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber o que é cada código, utilize o exemplo abaixo (no caso para VB)\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming é a técnica de remover sufixos e prefixos de uma palavra, chamada stem. \n",
    "\n",
    "Por exemplo, o stem da palavra cooking é cook. Um bom algoritmo sabe que \"ing\" é um sufixo e pode ser removido. \n",
    "\n",
    "Stemming é muito usado em mecanismos de buscas para indexação de palavras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Cria o Stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "# Para português: SnowballStemmer('portuguese')\n",
    "for word in word_tokens3:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que algumas palavras aparece apenas o stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatização na linguística, é o processo de agrupar as diferentes formas flexionadas de uma palavra para que possam ser analisadas como um único item.\n",
    "Na linguística computacional, a Lemmatização é o processo algorítmico de determinação do lema para uma determinada palavra. \n",
    "\n",
    "A Lemmatização está intimamente relacionada com o Stemming. \n",
    "\n",
    "A diferença é que um stemmer opera em uma única palavra sem conhecimento do contexto e, portanto, não pode discriminar entre palavras que têm diferentes significados, dependendo da parte da fala. No entanto, os stemmers são geralmente mais fáceis de implementar e executar mais rapidamente, e a precisão reduzida pode não ser importante para algumas aplicações.\n",
    "\n",
    "O Stemmning pode gerar palavras geralmente inexistentes, enquanto as lemas são palavras reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Vamos aplicar às palavras\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for word in word_tokens3:\n",
    "    print(wordnet_lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja a diferença do Lemmatization para o Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1\n",
    "\n",
    "## Crie um código Python que faça o seguinte:\n",
    "    \n",
    " 1) Reuna em duas variáveis de texto (textoPos, textoNeg) todas as palavras de todos os documentos da categoria 'pos' e 'neg' respectivamente\n",
    "\n",
    " 2) Execute os itens abaixo para textoPos e textoNeg\n",
    " \n",
    " 2.1) Aplique o método de tokenização para separar as palavras\n",
    " \n",
    " 2.2) Remova as stopwords e demais palavras que você julgar necessário para a limpeza do texto\n",
    " \n",
    " 2.3) Aplique a lematização\n",
    " \n",
    " 2.4) Calcule e imprima a frequência das 1000 palavras mais frequentes após todo o processamento \n",
    " \n",
    "Salve o Jupyter Notebook com os resultados e entregue pelo iLang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
