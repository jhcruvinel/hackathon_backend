{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neste lab vamos criar um classificador utilizando NLTK e Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o mesmo dataset de revisão de filmes disponível em http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "Segue o processo de criação do classificador:\n",
    "\n",
    "1) Vamos treinar modelos com avaliações positivas e negativas. Para isso vamos selecionar 70% dos dados para treinamento \n",
    "2) Depois que o modelo estiver treinado, vamos utilizar os 30% restantes para testar o modelo, passando apenas o texto e verificando se a previsão bateu com a categoria alvo (positiva ou negativa)\n",
    "\n",
    "Para esta prática baixamos no exercício anterior o dataset http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip\n",
    "\n",
    "Baixe e descompacte este arquivo zip. Se já fez isso no exercício anterior pode pular esse passo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando módulos novos\n",
    "! pip install numpy\n",
    "! pip install sklearn\n",
    "! pip install tensorflow\n",
    "! pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações Iniciais\n",
    "import os\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de variaveis globais\n",
    "FILES_DIR = 'tokens'\n",
    "MINIMO_LETRAS = 3\n",
    "PALAVRAS_IGNORADAS = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']','``',':','http','html','//members']\n",
    "NUMERO_PALAVRAS_MAIS_FREQUENTES = 1000\n",
    "PERCENTUAL_TESTE = 0.33\n",
    "PASSOS_TREINAMENTO = 100\n",
    "BATCH_SIZE = 50\n",
    "VALIDATION_SET = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos textos e categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "categorias = []\n",
    "documentos = []\n",
    "resultados = []\n",
    "i = 0\n",
    "for _, dirs, _ in os.walk(os.path.join(FILES_DIR)):\n",
    "    for d in dirs:\n",
    "        if d not in categorias:\n",
    "            categorias.append(d)\n",
    "        print('lendo categoria {}'.format(d))\n",
    "        for _, _, files in os.walk(os.path.join(FILES_DIR,d)):\n",
    "            for f in files:\n",
    "                i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print('{} arquivos lidos. lendo {}'.format(i,f))\n",
    "                with open(os.path.join(FILES_DIR,d,f), \"r\") as f:\n",
    "                     documentos.append(f.read())\n",
    "                resultados.append(d)\n",
    "end = time.time()                \n",
    "print ('\\nExistem {} textos e {} categorias: {}. \\nProcessamento em {}s'.format(len(documentos),len(categorias),categorias,end - start))                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação do texto\n",
    "\n",
    "O que vamos fazer em sequida é preparar o texto de cada arquivo, fazendo a tokenização, stopwords e lemmatization\n",
    "\n",
    "Além disso vamos calcular as 1000 palavras mais frequentes em todos os textos. Para isso precisamos de armazenar em uma variável todas as palavras que apareceram em todos os documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização de palavras\n",
    "start = time.time()\n",
    "documentos_tokenizados = []\n",
    "for documento in documentos:\n",
    "    documentos_tokenizados.append(word_tokenize(documento))    \n",
    "end = time.time()\n",
    "print ('{} documentos tokenizados. \\nProcessamento em {}s'.format(len(documentos_tokenizados),end - start))                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o primeiro documento após processamento\n",
    "print(documentos_tokenizados[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo Stop Words, lista de ignore e palavras menores que N letras\n",
    "# Stop words em inglês\n",
    "start = time.time()\n",
    "english_stops = set(stopwords.words('english'))\n",
    "documentos_sem_stop_words_e_ignore = []\n",
    "for documento in documentos_tokenizados:\n",
    "    # List comprehension para remover as stop words\n",
    "    aux = [w for w in documento if w not in english_stops]\n",
    "    # List comprehension para remover as demais palavras a serem ignoradas\n",
    "    aux2 = [w for w in aux if w not in PALAVRAS_IGNORADAS]\n",
    "    # List comprehension para remover palavras menores que N letras\n",
    "    documentos_sem_stop_words_e_ignore.append([w for w in aux2 if len(w) >= MINIMO_LETRAS])\n",
    "end = time.time()\n",
    "print ('{} documentos removidos as stop words e palavras ignoradas. \\nProcessamento em {}s'\n",
    "       .format(len(documentos_sem_stop_words_e_ignore),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o primeiro documento após processamento\n",
    "print(documentos_sem_stop_words_e_ignore[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processando o stemming para cada palavra, transformando em minísculo\n",
    "start = time.time()\n",
    "stemmer = SnowballStemmer('english')\n",
    "documentos_apos_stemming = []\n",
    "for documento in documentos_sem_stop_words_e_ignore:\n",
    "    # List comprehension para fazer o stemming de cada palavra\n",
    "    documentos_apos_stemming.append([stemmer.stem(w.lower()) for w in documento])\n",
    "end = time.time()\n",
    "print ('{} documentos após aplicar stemmer. \\nProcessamento em {}s'.format(len(documentos_apos_stemming),end - start))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o primeiro documento após processamento\n",
    "print(documentos_apos_stemming[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro vamos ter que criar uma lista de todas as palavras\n",
    "start = time.time()\n",
    "todas_palavras = []\n",
    "for documento in documentos_apos_stemming:\n",
    "    todas_palavras.extend(documento)\n",
    "end = time.time()\n",
    "print ('foram reunidas um todal de {} palavras. \\nProcessamento em {}s'.format(len(todas_palavras),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a frequência\n",
    "start = time.time()\n",
    "fdist = nltk.FreqDist(todas_palavras)\n",
    "palavras_mais_frequentes = []\n",
    "for word_freq in fdist.most_common(NUMERO_PALAVRAS_MAIS_FREQUENTES):\n",
    "    palavras_mais_frequentes.append(word_freq[0])\n",
    "palavras_mais_frequentes = sorted(list(set(palavras_mais_frequentes)))\n",
    "end = time.time()\n",
    "print('Número total de tokens distintos {}. \\nSelecionando apenas as {} palavras mais frequentes. \\nProcessamento em {}s'\n",
    "      .format(fdist.N(),NUMERO_PALAVRAS_MAIS_FREQUENTES,end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo apenas as 100 primeiras mais frequentes\n",
    "print(palavras_mais_frequentes[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "A partir de agora já temos:\n",
    "\n",
    "- palavras_mais_frequentes - lista de palavras mais frequentes\n",
    "- textos_apos_stemming - Lista de documentos após pré-processamento\n",
    "- resultados - Lista de resultados esperados\n",
    "- categorias - Lista das possíveis categorias\n",
    "\n",
    "Com isso, podemos agora criar um \"saco de palavras\" (bag of words) considerando apenas as N palavras mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar o BagOfWords (BoW) para os documentos, \n",
    "# considerando apenas as N palavras mais frequentes e ignorando as demais\n",
    "start = time.time()\n",
    "documentos_bow = []\n",
    "resultados_bow = []\n",
    "for i in range(len(documentos_apos_stemming)):\n",
    "    documento = documentos_apos_stemming[i]\n",
    "    resultado = resultados[i]\n",
    "    documento_bow = []\n",
    "    # Cria um array com o bag of words de N palavras mais frequentes, colocando 1 quando a palavra estiver presente no documento\n",
    "    for w in palavras_mais_frequentes:\n",
    "        documento_bow.append(1) if w in documento else documento_bow.append(0)\n",
    "    documentos_bow.append(documento_bow)\n",
    "    # Inicializa o array de resultados com zero\n",
    "    resultado_bow = list([0] * len(categorias))\n",
    "    # Coloca 1 na posição do array correta de acordo com o resultado\n",
    "    resultado_bow[categorias.index(resultado)] = 1\n",
    "    resultados_bow.append(resultado_bow)\n",
    "end = time.time()\n",
    "print('Criou um total de {} documentos BoW. \\nProcessamento em {}s'.format(len(documentos_bow),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o primeiro documento BoW\n",
    "print('primeiro documento {}'.format(documentos_bow[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o primeiro documento BoW\n",
    "print('primeiro resultado BoW {} para as categorias {}'.format(resultados_bow[0], categorias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo para array Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Em seguida vamos converter para o formato nparray aceito pelo TensorFlow\n",
    "documentos_bow = np.array(documentos_bow)\n",
    "resultados_bow = np.array(resultados_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando dados de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(documentos_bow, resultados_bow, test_size=PERCENTUAL_TESTE, random_state=42)\n",
    "print('Separando {} documentos/resultados para treinamento e {} para teste'.format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo da Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criando a Rede Neural utilizando a biblioteca TFLearn\n",
    "# Reset do grafo\n",
    "tf.reset_default_graph()\n",
    "# Cria a rede neural\n",
    "net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "# Define o modelo e configura o tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs/'+time.strftime(\"%Y%m%d_%H%M\"))\n",
    "print('Modelo de rede neural criado: {}'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando o Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "model.fit(X_train, y_train, n_epoch=PASSOS_TREINAMENTO, batch_size=BATCH_SIZE, validation_set=VALIDATION_SET, show_metric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "Abra o **TensorBoard** utilizando o comando abaixo em um prompt do Python para acompanhar o treinamento.\n",
    "\n",
    "Veja os gráficos de custo e acurácia gerados:\n",
    "    \n",
    "    python -m tensorboard.main --logdir=\"tflearn_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando o modelo treinado com os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test accuarcy: %0.4f%%' % (score[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fazendo a inferência para um caso apenas\n",
    "prediction = model.predict([X_test[0]])\n",
    "print(\"Valor previsto: {}. \\nValor esperado: {}\".format(prediction[0],y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2\n",
    "\n",
    "## Melhore o percentual do classificador \n",
    "    \n",
    "Como pode ver no exemplo acima, o resultado do treinamento ainda não está com um percentual bom.\n",
    "\n",
    "Realize as mudanças abaixo para tentar alcançar um perncentual melhor para o resultado da avaliação com os dados de teste\n",
    "\n",
    "1) Faça alterações nos parâmetros abaixo e anote o resultado final de acurácia em uma tabela\n",
    "\n",
    "    MINIMO_LETRAS\n",
    "    PALAVRAS_IGNORADAS\n",
    "    NUMERO_PALAVRAS_MAIS_FREQUENTES\n",
    "    PERCENTUAL_TESTE\n",
    "    PASSOS_TREINAMENTO\n",
    "    BATCH_SIZE\n",
    "    VALIDATION_SET\n",
    "\n",
    " 2) Mude o desenho da rede neural e anote o resultado final de acurácia em uma tabela  \n",
    "\n",
    "Salve o Jupyter Notebook com os resultados e entregue pelo iLang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabela de resultados finais\n",
    "\n",
    "Exemplo de tabela de resultado a ser gerada com valores fictícios:\n",
    "\n",
    "| **Parâmetros alterados** | **Accuracy** |\n",
    "| --- | --- | \n",
    "| VALORES INICIAIS | 69% |\n",
    "| PASSOS_TREINAMENTO = 200 | 70% |\n",
    "| PASSOS_TREINAMENTO = 200, BATCH_SIZE = 20 | 71% |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
