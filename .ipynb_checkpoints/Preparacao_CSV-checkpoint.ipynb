{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install psycopg2\n",
    "#!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports iniciais\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dados_teste.csv',delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4721.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2818.420545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2219.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>4693.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7108.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9995.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            numero\n",
       "count   400.000000\n",
       "mean   4721.890000\n",
       "std    2818.420545\n",
       "min      24.000000\n",
       "25%    2219.500000\n",
       "50%    4693.500000\n",
       "75%    7108.750000\n",
       "max    9995.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nome        400\n",
       "empresa     400\n",
       "endereco    400\n",
       "data1       400\n",
       "data2       400\n",
       "numero      400\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "ignore = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']','``',':','http','html','//members']\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Cria o Stemmer\n",
    "stemmer = SnowballStemmer('portuguese')\n",
    "def preprocess_text(content):\n",
    "    content.translate(str.maketrans('', '', string.punctuation))\n",
    "    word_tokens = nltk.word_tokenize(content)\n",
    "    #print(len(word_tokens),word_tokens)\n",
    "    word_tokens2 = [w.lower() for w in word_tokens if w not in portuguese_stops]\n",
    "    #print(len(word_tokens2),word_tokens2)\n",
    "    word_tokens3 = [w for w in word_tokens2 if w not in ignore]\n",
    "    #print(len(word_tokens3),word_tokens3)\n",
    "    word_tokens4 = [stemmer.stem(w) for w in word_tokens3]\n",
    "    #print(len(word_tokens4),word_tokens4)\n",
    "    return ' '.join(word_tokens4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = 'Teste de sentença para tokenizar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test sentenc tokeniz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      anjoli q. campbell\n",
       "1        deann f. burnett\n",
       "2             col z. trav\n",
       "3        sony y. mckinney\n",
       "4           logan v. holm\n",
       "              ...        \n",
       "395      olga i. thornton\n",
       "396           xen k. stok\n",
       "397      cassidy k. potts\n",
       "398          kyle p. robl\n",
       "399        jarrod n. robl\n",
       "Name: nome, Length: 400, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nome'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a frequência\n",
    "\n",
    "Calcula quantas vezes uma palavra aparece em um texto ou corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos primeiro \n",
    "fdist = nltk.FreqDist(word_tokens)\n",
    "print('\\nContagem do número máximo de ocorrências do token \"',fdist.max(),'\" : ', fdist[fdist.max()])\n",
    "print('\\nNúmero total de tokens distintos : ', fdist.N())\n",
    "print('\\nA seguir estão os 10 tokens mais comuns')\n",
    "print(fdist.most_common(40))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Stopwords são palavras comuns que normalmente não contribuem para o significado de uma frase e que por isso podem ser ignoradas em processamento PLN. São palavras como \"The\" e \"a\" ((em inglês) ou \"O/A\" e \"Um/Uma\" ((em português). \n",
    "\n",
    "Muitos mecanismos de busca filtram estas palavras (stopwords), como forma de economizar espaço em seus índices de pesquisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que aqui temos menos palavras, pois as stopwords foram removidas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos também criar uma lista de caracteres palavras que devem ser ignoradas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que conseguimor limpar ainda mais as palavras a serem trabalhadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging\n",
    "\n",
    "O POS Tagging é o processo de rotulação de elementos textuais - tipicamente palavras e pontuação - com o fim de evidenciar \n",
    "a estrutura gramatical de um determinado trecho de texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "tags = pos_tag(word_tokens3)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber o que é cada código, utilize o exemplo abaixo (no caso para VB)\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming é a técnica de remover sufixos e prefixos de uma palavra, chamada stem. \n",
    "\n",
    "Por exemplo, o stem da palavra cooking é cook. Um bom algoritmo sabe que \"ing\" é um sufixo e pode ser removido. \n",
    "\n",
    "Stemming é muito usado em mecanismos de buscas para indexação de palavras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Cria o Stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "# Para português: SnowballStemmer('portuguese')\n",
    "for word in word_tokens3:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que algumas palavras aparece apenas o stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatização na linguística, é o processo de agrupar as diferentes formas flexionadas de uma palavra para que possam ser analisadas como um único item.\n",
    "Na linguística computacional, a Lemmatização é o processo algorítmico de determinação do lema para uma determinada palavra. \n",
    "\n",
    "A Lemmatização está intimamente relacionada com o Stemming. \n",
    "\n",
    "A diferença é que um stemmer opera em uma única palavra sem conhecimento do contexto e, portanto, não pode discriminar entre palavras que têm diferentes significados, dependendo da parte da fala. No entanto, os stemmers são geralmente mais fáceis de implementar e executar mais rapidamente, e a precisão reduzida pode não ser importante para algumas aplicações.\n",
    "\n",
    "O Stemmning pode gerar palavras geralmente inexistentes, enquanto as lemas são palavras reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Vamos aplicar às palavras\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for word in word_tokens3:\n",
    "    print(wordnet_lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja a diferença do Lemmatization para o Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1\n",
    "\n",
    "## Crie um código Python que faça o seguinte:\n",
    "    \n",
    " 1) Reuna em duas variáveis de texto (textoPos, textoNeg) todas as palavras de todos os documentos da categoria 'pos' e 'neg' respectivamente\n",
    "\n",
    " 2) Execute os itens abaixo para textoPos e textoNeg\n",
    " \n",
    " 2.1) Aplique o método de tokenização para separar as palavras\n",
    " \n",
    " 2.2) Remova as stopwords e demais palavras que você julgar necessário para a limpeza do texto\n",
    " \n",
    " 2.3) Aplique a lematização\n",
    " \n",
    " 2.4) Calcule e imprima a frequência das 1000 palavras mais frequentes após todo o processamento \n",
    " \n",
    "Salve o Jupyter Notebook com os resultados e entregue pelo iLang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
